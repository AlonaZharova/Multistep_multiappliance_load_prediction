{"cells":[{"cell_type":"markdown","metadata":{"id":"JW8q2Hxl3thA"},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/bads/blob/master/tutorials/9_nb_feature_engineering.ipynb) "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1660,"status":"ok","timestamp":1650666556364,"user":{"displayName":"Antonia Deeplearning","userId":"03469055858719879397"},"user_tz":-120},"id":"u5McaQPrIFbW","outputId":"0ed7e412-c30a-4592-d935-f34b92d46e32"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/code\n","!pip install --upgrade pip\n","!pip install -r requirements.txt\n"],"metadata":{"id":"olSVLv4WYbqH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dC0hxs_4FIQr"},"source":["### Parameters and Settings"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wz7fwPe8mgNU"},"outputs":[],"source":["# Import standard Python libraries\n","\n","\n","import numpy as np\n","import pandas as pd\n","import time\n","import json\n","import os\n","import random\n","import joblib\n","\n","from sklearn.model_selection import train_test_split\n","#from sklearn.datasets import fetch_covtype\n","from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics.pairwise import pairwise_kernels\n","from tqdm.notebook import tqdm\n","from os import path\n","import warnings\n","from pickle import load\n","warnings.filterwarnings(\"ignore\")\n","\n","import keras\n","import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense, Input, LSTM, Concatenate\n","from tensorflow.keras.layers import Dropout, Bidirectional, Flatten\n","from tensorflow.keras.layers import Conv1D, Conv2D\n","from tensorflow.keras.layers import MaxPooling1D, Embedding\n","from tensorflow.keras.layers import RepeatVector, Reshape\n","from tensorflow.keras.layers import TimeDistributed\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import BatchNormalization\n","#from sklearn.neural_network import MLPRegressor\n","from tensorflow.keras.layers import Activation\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","\n","import xgboost as xgb\n"]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/code/vest-python-master/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J6HVqNqWYdii","executionInfo":{"status":"ok","timestamp":1650666566003,"user_tz":-120,"elapsed":9,"user":{"displayName":"Antonia Deeplearning","userId":"03469055858719879397"}},"outputId":"937ce436-82d6-420e-dfe6-172ed1cd3119"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/code/vest-python-master\n"]}]},{"cell_type":"code","source":["from gtda.time_series import TakensEmbedding\n","\n","from vest.models.univariate import UnivariateVEST\n","from vest.config.aggregation_functions \\\n","    import (SUMMARY_OPERATIONS_ALL,\n","            SUMMARY_OPERATIONS_FAST,\n","            SUMMARY_OPERATIONS_SMALL)\n","# In case of an error-message like following:\n","# ContextualVersionConflict: (scipy 1.4.1 (/usr/local/lib/python3.7/dist-packages), Requirement.parse('scipy>=1.5'), {'stumpy'})\n","# replace the float_factorial function in the scipy package (most likely in this location: /usr/local/lib/python3.7/dist-packages/scipy/_lib/_util.py )\n","# with the following function:\n","\n","#def float_factorial(num):\n","#    \"\"\"Compute the factorial and return as a float\n","#\n","#    Returns infinity when result is too large for a double\n","#    \"\"\"\n","#    val = float(math.factorial(num)) if num < 171 else np.inf\n","#    return val\n","\n","# Afterwards restart runtime and run code again."],"metadata":{"id":"hfTyTu3yMSqg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vp-ZbYIx7IGr"},"source":["#### Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i42dlocX7IGs"},"outputs":[],"source":["def get_accuracy(y_real, y_pred):\n","  deviation_pred = np.abs((y_real - y_pred))\n","  dev_in_percent = (deviation_pred/100)*np.abs(y_real)\n","  dev_in_percent = np.where(dev_in_percent==0, 5, dev_in_percent)\n","  acc_5_lst = np.where(dev_in_percent>5, 0, 1)\n","  acc_5 = acc_5_lst.sum()/len(y_real)\n","  return acc_5\n","\n","def get_accuracy_90(y_real, y_pred):\n","  deviation_pred = np.abs((y_real - y_pred))\n","  dev_in_percent = (deviation_pred/100)*np.abs(y_real)\n","  dev_in_percent = np.where(dev_in_percent==0, 5, dev_in_percent)\n","  acc_5_lst = np.where(dev_in_percent>5, 0, 1)\n","  acc_10 = acc_5_lst.sum()/len(y_real)\n","  return acc_10\n","\n","def get_wmape(y_real, y_pred):\n","  y_real, y_pred = np.array(y_real), np.array(y_pred) \n","  return np.sum(np.abs((y_real - y_pred))) / np.sum(np.abs(y_real))\n","\n","\n","def get_mase_onestep(y_real, y_pred, y_train):\n","    \"\"\"\n","    Computes the MEAN-ABSOLUTE SCALED ERROR forcast error for univariate time series prediction.\n","    \n","    See \"Another look at measures of forecast accuracy\", Rob J Hyndman\n","    \n","    parameters:\n","        training_series: the series used to train the model, 1d numpy array\n","        testing_series: the test series to predict, 1d numpy array or float\n","        prediction_series: the prediction of testing_series, 1d numpy array (same size as testing_series) or float\n","        absolute: \"squares\" to use sum of squares and root the result, \"absolute\" to use absolute values.\n","    \n","    \"\"\"\n","    n = y_train.shape[0]\n","    d = np.abs(np.diff(y_train)).sum()/(n-1)\n","    \n","    errors = np.abs(y_real - y_pred )\n","    return errors.mean()/d\n","\n","def get_mase(actual: np.ndarray, predicted: np.ndarray, naive: np.ndarray):\n","    \"\"\"\n","    Mean Absolute Scaled Error\n","    Baseline (benchmark) is computed with naive forecasting 24 h\n","    \"\"\"\n","    return mean_absolute_error(actual, predicted) / mean_absolute_error(actual, naive)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g2f65Xct7IGv"},"outputs":[],"source":["# Formulas to get all evaluation metrics for flattened and unflattened predictions\n","def evaluate_predictions_flat(actual, predicted, test_y=None, testing=True):\n","\n","    if (actual.shape[1] != 1) or (predicted.shape[1] != 1):\n","      raise AssertionError('Second dimension of actual and predicted array must be 1')\n","\n","    rmse_flat = mean_squared_error(actual, predicted, squared=False)\n","    nrmse_flat = np.asscalar(mean_squared_error(actual, predicted, squared=False)/(max(actual)-min(actual)))\n","    mae_flat = mean_absolute_error(actual, predicted)\n","    wmape_flat = get_wmape(actual, predicted)\n","    mase_flat = get_mase(actual, predicted, test_y)\n","    acc_flat = get_accuracy(actual, predicted)\n","    acc_90_flat = get_accuracy_90(actual, predicted)\n","\n","    print('RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY')\n","    print(f'{rmse_flat:.2f},{mae_flat:.2f},{wmape_flat:.2f},{mase_flat:.2f}, {nrmse_flat:.2f}, {acc_flat:.2f}')\n","\n","    if not round(wmape_flat,0)==0 and testing==True:\n","      raise AssertionError()\n","\n","    return [rmse_flat, mae_flat, wmape_flat, mase_flat, nrmse_flat, acc_flat, acc_90_flat]\n","\n","def evaluate_predictions_windows(actual, predicted, test_y=None, testing=True):\n","\n","    wmapes_24 = []\n","    rmse_24 = []\n","    nrmse_24 = []\n","    mae_24 = []\n","    mase_24 = []\n","    acc_24 = []\n","    acc_90_24 = []\n","\n","    for i in range(actual.shape[0]):\n","      rmses = mean_squared_error(actual[i], predicted[i], squared=False)\n","      nrmses = mean_squared_error(actual[i], predicted[i], squared=False)/(max(actual[i])-min(actual[i]))\n","      maes = mean_absolute_error(actual[i], predicted[i])\n","      wmapes = get_wmape(actual[i], predicted[i])\n","      mases = get_mase(actual[i], predicted[i], test_y[i])\n","      accs = get_accuracy(actual[i], predicted[i])\n","      accs_90 = get_accuracy_90(actual[i], predicted[i])\n","\n","      rmse_24.append(rmses)\n","      nrmse_24.append(nrmses)\n","      mae_24.append(maes)\n","      wmapes_24.append(wmapes)\n","      mase_24.append(mases)\n","      acc_24.append(accs)\n","      acc_90_24.append(accs_90)\n","\n","    if not round(np.mean(np.ma.masked_invalid(wmapes_24)),0)==0 and testing==True:\n","      raise AssertionError()\n","\n","    return [rmse_24, mae_24, wmapes_24, mase_24, nrmse_24, acc_24, acc_90_24]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_LjQ4Yb97IGx"},"outputs":[],"source":["# calculate differences to original data for many models:\n","def print_originalvalues_results(result_list, y_test_control, naive, originals=True):\n","  \n","  rmse_lst = []\n","  mae_lst = []\n","  wmape_lst = []\n","  mase_lst = []\n","  nrmse_lst = []\n","  accur_lst = []\n","  accur_90_lst = []\n","\n","  for i in range(len(result_list)):\n","    if originals == True:\n","      y_pred_fridge = preproc_target.inverse_transform(result_list[i].reshape((-1,1)))\n","      metric_list = evaluate_predictions_flat(y_test_control, y_pred_fridge, naive, testing=False)\n","    else:\n","      scaled_true = preproc_target.transform(y_test_control)\n","      scaled_naive = preproc_target.transform(naive)\n","      metric_list = evaluate_predictions_flat(scaled_true, result_list[i].reshape((-1,1)), scaled_naive, testing=False)\n","\n","    \n","    rmse_lst.append(metric_list[0])\n","    mae_lst.append(metric_list[1])\n","    wmape_lst.append(metric_list[2])\n","    mase_lst.append(metric_list[3])\n","    nrmse_lst.append(metric_list[4])\n","    accur_lst.append(metric_list[5])\n","    accur_90_lst.append(metric_list[6])\n","\n","  return rmse_lst, mae_lst, wmape_lst, mase_lst, nrmse_lst, accur_lst, accur_90_lst"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJk35ian7IGy"},"outputs":[],"source":["# Fil empty dataframe with the time lag observations per variable as features for 24h\n","def get_tlags_rolling_features(df, col_names, tlag):\n","\n","   dataframe = pd.DataFrame()\n","   for col in col_names:\n","     for i in range((tlag-1),0,-1):\n","       dataframe[col + '_t-'+str(i)] = df[col].shift(i).values[:]\n","     dataframe[col] = df[col]\n","   \n","   dataframe = dataframe[(tlag-1):]\n","   dataframe.dropna(inplace=True)\n","   dataframe.reset_index(drop=True, inplace=True)\n","   \n","   nparray = np.empty((dataframe.shape[0], tlag, len(col_names)))\n","   for i in range(len(col_names)):\n","      nparray[:,:,i] = dataframe.values[:,i*tlag:(i+1)*tlag]\n","   return dataframe, nparray"]},{"cell_type":"code","source":["def get_msvr_gamma(xtrain, relational=True, factor=10):\n","\n","  if relational==True:\n","    msvr_gamma = 1 / (xtrain.shape[1] * xtrain.var())\n","\n","  else:\n","    msvr_gamma = 1 / factor\n","  \n","  return msvr_gamma"],"metadata":{"id":"G9P2Mjio1KY5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Specify Experiment Parameters"],"metadata":{"id":"Yb4BwqNLUNEM"}},{"cell_type":"code","source":["# Load Data\n","dataset = 'refit' # 'refit' 'ampds' , 'pecansd', 'greend'\n","dimension_var = 'many' # define wether it will be the baseline or a feature run-through\n","ylag = 24 # define number of time steps to predict\n","target = 'Fridge' # Specify the target Appliance\n","feature_group = 'baseline' \n","# Options. 'baseline', 'appliances', 'weather', 'sinecosine', 'lon_loff', 'all', \n","# 'weather_sinecosine', 'autoreg_features', 'interact_features', 'vest_features', 'taken_features'\n","path_to_data_folder = '/content/drive/MyDrive/'\n","\n","if dataset == 'refit':\n","  data_url = f'{path_to_data_folder}data/REFIT/'\n","  reshape_dimension = 113\n","elif dataset == 'ampds':\n","  data_url = f'{path_to_data_folder}data/AMPds/'\n","  reshape_dimension = 113\n","elif dataset == 'pecansd':\n","  data_url = f'{path_to_data_folder}data/PecanSD/'\n","  reshape_dimension = 86\n","elif dataset == 'greend':\n","  data_url = f'{path_to_data_folder}data/GreenD/'\n","  reshape_dimension = 86  \n","else:\n","  raise AssertionError('Please specify a valid dataset')\n","\n","df = pd.read_csv(f'{data_url}traindata_scaled.csv')\n","\n","nparray = np.loadtxt(f'{data_url}traindata_scaled.txt')\n","nparray = nparray.reshape((-1, ylag, reshape_dimension))\n","print(nparray.shape)\n","\n","# train and valid set params\n","split_ratio = 0.2\n","val_split = int(np.floor(len(df)*split_ratio))\n","target_idx = list(df.columns).index(target)-1\n","\n","# FE Selection Features:\n","if feature_group == 'baseline':\n","  col_idx = np.r_[target_idx:target_idx+1]\n","  dimension_var = 'one'\n","elif feature_group == 'appliances':\n","  if dataset in ('refit', 'ampds'):\n","    # Nur Appliances:\n","    col_idx = np.r_[0:5]\n","  elif dataset in ('pecansd', 'greend'):\n","    col_idx = np.r_[0:4]\n"," \n","elif feature_group == 'weather': \n","  if dataset in ('refit', 'ampds'):\n","    col_idx = np.r_[target_idx:target_idx+1,5:8]\n","  elif dataset in ('pecansd', 'greend'):\n","    col_idx = np.r_[target_idx:target_idx+1,4:7]\n","\n","elif feature_group == 'sinecosine': \n","  if dataset in ('refit', 'ampds'):\n","    col_idx = np.r_[target_idx:target_idx+1,90:91, 95:103]\n","  elif dataset in ('pecansd', 'greend'):\n","    col_idx = np.r_[target_idx:target_idx+1, 65:66, 70:78]\n","\n","elif feature_group == 'lon_loff': \n","  if dataset in ('refit', 'ampds'):\n","    col_idx = np.r_[target_idx:target_idx+1, 103+target_idx:105+target_idx+1]\n","  elif dataset in ('pecansd', 'greend'):\n","    col_idx = np.r_[target_idx:target_idx+1, 78+target_idx:80+target_idx+1]\n","\n","elif feature_group == 'all': \n","  if dataset in ('refit', 'ampds'):\n","    col_idx = np.r_[0:91, 95:113]\n","  elif dataset in ('pecansd', 'greend'):\n","    col_idx = np.r_[0:66, 70:86] \n","\n","elif feature_group == 'weather_sinecosine': \n","  if dataset in ('refit', 'ampds'):\n","    col_idx = np.r_[target_idx:target_idx+1, 90:91, 95:103]\n","  elif dataset in ('pecansd', 'greend'):\n","    col_idx = np.r_[target_idx:target_idx+1,65:66, 70:78] \n","elif feature_group =='autoreg_features': \n","  autoreg_features = True\n","elif feature_group =='interact_features':\n","  interact_features = True\n","elif feature_group =='vest_features':\n","  vest_features = True\n","elif feature_group =='taken_features':\n","  taken_features = True  \n","else:\n","  raise AssertionError('Please specify a valid feature group')\n","\n","\n","# Model Paramteres\n","epochs = 10\n","loss = 'mse'\n","optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n","\n","drpt_rate = 0.06\n","hidd_dim = 172\n","lr_alpha = 0.2\n","\n","# XGBoost\n","num_round = 750\n","\n","# Leave most parameters as default\n","param = {'objective': 'reg:squarederror', # Specify multiclass classification\n","         'tree_method': 'gpu_hist' # Use GPU accelerated algorithm\n","         }\n","#param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}\n","\n","# MSVR\n","msvr_kernel = 'rbf'\n","msvr_relational = True # describe wofür param ist:\n","msvr_factor = 10 # describe wofür das ist\n","msvr_epsilon=0.1 \n","msvr_degree=2\n","msvr_c=0.1\n","\n","# LSTM\n","\n","#Seq2Seq context\n","hidd_dim_context = 281\n","\n","# FFNN:\n","hidd_dim_ffnn = 182\n","batch_size = 64\n","\n","#CNN-LSTM\n","hidd_dim_cnn_lstm = 146\n","conv_filter = 64\n","drpt_rate_cnn = 0.36 # check ob das im paper steht sonst anpassen\n","pl_size = 2\n","kernel_size = 2\n","\n","# Evaluation Dictionaryy:\n","evaluation_dict = {}\n"],"metadata":{"id":"PCXjCnqPUMTU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650666595198,"user_tz":-120,"elapsed":29201,"user":{"displayName":"Antonia Deeplearning","userId":"03469055858719879397"}},"outputId":"ad806b39-9492-405a-d315-fc38bebd446a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(9330, 24, 113)\n"]}]},{"cell_type":"markdown","metadata":{"id":"zaAo3yIQbvhu"},"source":["### Data Selection and Feature Groups"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":330,"status":"ok","timestamp":1650666595524,"user":{"displayName":"Antonia Deeplearning","userId":"03469055858719879397"},"user_tz":-120},"id":"oAVpNsZdbtfr","outputId":"e37ffb72-bbf8-477e-f6bd-40d1ae7eb30b"},"outputs":[{"output_type":"stream","name":"stdout","text":["RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","0.00,0.00,0.00,0.00, 0.00, 1.00\n"]}],"source":["Xtrain_arrayfe = nparray[:-val_split]\n","ytrain_arrayfe = nparray[ylag:-val_split+ylag, :, target_idx]\n","\n","Xvalid_arrayfe = nparray[-val_split:-ylag]\n","yvalid_arrayfe = nparray[-val_split+ylag:, :, target_idx]\n","\n","# get scalars and test transformation process for evaluation\n","preproc_target = load(open(f'{data_url}std_scaler_{target}.pkl', 'rb'))\n","df_tr = pd.read_csv(f'{data_url}original_traindata_imputed_outlreplaced.csv')\n","\n","# inverse transform our predictions:\n","y_valid_fridge = yvalid_arrayfe[::24, :].reshape((-1,1))\n","y_valid_fridge = preproc_target.inverse_transform(y_valid_fridge)\n","\n","#inverse transform test values\n","Xvalid_fridge = Xvalid_arrayfe[::24,:,target_idx].reshape((-1,1))\n","Xvalid_fridge = preproc_target.inverse_transform(Xvalid_fridge)\n","\n","# get part of original data that we use\n","df_valid_true = df_tr[target].iloc[-len(y_valid_fridge)-11:-11].values.reshape((-1,1)) # refit = -3/ smart -13/ ampds -13\n","\n","# calculate difference to original data\n","rmse, mae, mape_sin_nan, mase, nrmse, acc, acc_90 = evaluate_predictions_flat(df_valid_true, y_valid_fridge, test_y= Xvalid_fridge)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":969,"status":"ok","timestamp":1650666596492,"user":{"displayName":"Antonia Deeplearning","userId":"03469055858719879397"},"user_tz":-120},"id":"rQJlS0ndfFdw","outputId":"52872742-b6a3-45d3-c686-a09f7ac3c84c"},"outputs":[{"output_type":"stream","name":"stdout","text":["RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","0.00,0.00,0.00,0.00, 0.00, 1.00\n"]}],"source":["# Load Test Data\n","test_file = f'{data_url}testdata_scaled.txt'\n","if path.exists(test_file):\n","  Xtest_array = np.loadtxt(test_file)\n","else:\n","  raise AssertionError(f\"File does not exist run notebook data_preparation before with dataset = {dataset} and train_test = 'test\")\n","\n","Xtest_array = Xtest_array.reshape((-1,24,reshape_dimension)) \n","Xtest = Xtest_array[:-1, :, :].copy()\n","ytest = Xtest_array[1:, :, target_idx].copy()\n","\n","# Check for Testdata:\n","df_t = pd.read_csv(f'{data_url}original_testdata_imputed_outlreplaced.csv')\n","\n","y_test_fridge = ytest.reshape((-1,1))\n","y_test_fridge = preproc_target.inverse_transform(y_test_fridge)\n","y_test_fridge_wind = y_test_fridge.reshape(ytest.shape)\n","\n","#inverse transform test values\n","Xtest_fridge = Xtest[:,:,target_idx].reshape((-1,1))\n","Xtest_fridge = preproc_target.inverse_transform(Xtest_fridge)\n","Xtest_fridge_wind = Xtest_fridge.reshape(Xtest[:,:,0].shape)\n","\n","# get part of original data that we use\n","df_test_true = df_t[target].iloc[-len(y_test_fridge)-24:-24].values.reshape((-1,1)) # refit -24  / Ampds -24\n","df_test_true_wind = df_test_true.reshape(Xtest[:,:,0].shape)\n","\n","# calculate difference to original data\n","rmse, mae, mape_sin_nan, mase, nrmse, acc, acc_90 = evaluate_predictions_flat(df_test_true, y_test_fridge, test_y = Xtest_fridge)"]},{"cell_type":"markdown","metadata":{"id":"gybi688wcdn9"},"source":["#### Feature Group Selection"]},{"cell_type":"markdown","metadata":{"id":"2Nvti6BqQehd"},"source":["##### Onehot encoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xyHrcHO7QgxF"},"outputs":[],"source":["#Nur Fridge\n","#nparray_train = np.loadtxt(f'{data_url}traindata/traindata_onehot.txt')\n","#nparray_train = nparray_train.reshape((-1, ylag, 48))\n","#Xtrain_arrayfe = nparray_train[:-val_split]\n","#\n","#Xvalid_arrayfe = nparray_train[-val_split:-ylag]\n","#\n","## Load Test Data\n","#nparray_test = np.loadtxt(f'{data_url}testdata/testdata_onehot.txt')\n","#nparray_test = nparray_test.reshape((-1,24,48)) \n","#Xtest = nparray_test[1:-1, :, :].copy()\n","#\n","#X_train = Xtrain_arrayfe.reshape((Xtrain_arrayfe.shape[0],-1))\n","#X_test = Xtest.reshape((Xtest.shape[0],-1))\n","#\n","#print(f'Shape of train and val array: {Xtrain_arrayfe.shape}, {Xvalid_arrayfe.shape}')\n","#print(f'Shape of train and val y array: {ytrain_arrayfe.shape}, {yvalid_arrayfe.shape}')\n","#print(f'Shape of test data X-array and y-array: {Xtest.shape}, {ytest.shape}') "]},{"cell_type":"markdown","metadata":{"id":"_xlUinQcAA5m"},"source":["##### Keep Feature Group Selection"]},{"cell_type":"code","source":["# Target timeseries + past stat summary features \n","if 'autoreg_features' in locals():\n","  candidate_list = [s for s in df.columns.to_list() if target in s]\n","  fe_list = candidate_list[:9]\n","# Fridge timeseries + past summary features with other appliances (interaction variables)\n","elif 'interact_features' in locals():\n","    candidate_list = [s for s in df.columns.to_list() if target in s]\n","    fe_list = candidate_list[9:-2]\n","    fe_list.insert(0,candidate_list[target_idx])\n","else:\n","  pass\n","\n","if 'fe_list' in locals():\n","  col_idx = []\n","  for col in fe_list:\n","    pos = df.columns.tolist().index(col)-1\n","    col_idx.append(pos)\n","  col_idx.sort()"],"metadata":{"id":"D5jE-hpaXokH"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1650666596493,"user":{"displayName":"Antonia Deeplearning","userId":"03469055858719879397"},"user_tz":-120},"id":"PxXmE_8xER3W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b4ed915b-e827-46ca-f906-14807524d3c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of train and val array: (7446, 24, 1), (1860, 24, 1)\n","Shape of train and val y array: (7446, 24), (1860, 24)\n","Shape of test data X-array and y-array: (112, 24, 1), (112, 24)\n","(7446, 24)\n","(112, 24)\n"]}],"source":["Xtrain_arrayfe = Xtrain_arrayfe[:,:,col_idx]\n","Xvalid_arrayfe = Xvalid_arrayfe[:,:,col_idx]\n","Xtest = Xtest[:,:,col_idx]\n","\n","# Input XGBoost, MSVR and FFNN:\n","X_train = Xtrain_arrayfe.reshape((Xtrain_arrayfe.shape[0],-1))\n","X_valid = Xvalid_arrayfe.reshape((Xvalid_arrayfe.shape[0],-1))\n","X_test = Xtest.reshape((Xtest.shape[0],-1))\n","\n","print(f'Shape of train and val array: {Xtrain_arrayfe.shape}, {Xvalid_arrayfe.shape}')\n","print(f'Shape of train and val y array: {ytrain_arrayfe.shape}, {yvalid_arrayfe.shape}')\n","print(f'Shape of test data X-array and y-array: {Xtest.shape}, {ytest.shape}') \n","print(X_train.shape)\n","print(X_test.shape)"]},{"cell_type":"code","source":["# Feature shapes for Seq2Seq context:\n","if dimension_var == 'one':\n","  # encoder inputs:\n","  Xtrain_arrayfe_enc = Xtrain_arrayfe[:,:,:]\n","  Xvalid_arrayfe_enc = Xvalid_arrayfe[:,:,:]\n","  Xtest_enc = Xtest[:,:,:]\n","else:\n","  # encoder inputs:\n","  Xtrain_arrayfe_enc = Xtrain_arrayfe[:,:,1:]\n","  Xvalid_arrayfe_enc = Xvalid_arrayfe[:,:,1:]\n","  Xtest_enc = Xtest[:,:,1:]\n","\n","# decoder output:\n","Xtrain_arrayfe_dec = Xtrain_arrayfe[:,:,0:1]\n","Xvalid_arrayfe_dec = Xvalid_arrayfe[:,:,0:1]\n","Xtest_dec = Xtest[:,:,0:1]\n","\n","\n","# Feature Shapes for CNN Input Layer:\n","Xtrain_arrayfe_cnn_lstm = Xtrain_arrayfe.reshape((Xtrain_arrayfe.shape[0],Xtrain_arrayfe.shape[1],Xtrain_arrayfe.shape[2],1))\n","Xvalid_arrayfe_cnn_lstm = Xvalid_arrayfe.reshape((Xvalid_arrayfe.shape[0],Xvalid_arrayfe.shape[1],Xvalid_arrayfe.shape[2],1))\n","Xtest_cnn_lstm = Xtest.reshape((Xtest.shape[0],Xtest.shape[1],Xtest.shape[2],1))"],"metadata":{"id":"ua3E0hbQ4nnb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kdiZiISmPHWB"},"source":["##### VEST Autoregressive features and ts transformations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzDwx6qEPLZ_"},"outputs":[],"source":["if 'vest_features' in locals():\n","  vest_model = UnivariateVEST()\n","\n","  vest_model.fit(X=Xtrain_arrayfe[:,:,0],\n","            correlation_thr=0.8,\n","            apply_transform_operators=True,\n","            summary_operators=SUMMARY_OPERATIONS_SMALL)\n","\n","  vestfe_train = vest_model.dynamics\n","  vestfe_train_ar = vestfe_train.values\n","  X_tr_augmented = np.concatenate((Xtrain_arrayfe[:,:,0], vestfe_train_ar), axis=1)\n","\n","  vestfe_valid = vest_model.transform(Xvalid_arrayfe[:,:,0])\n","  vestfe_valid_ar = vestfe_valid.values\n","  X_vl_augmented = np.concatenate((Xvalid_arrayfe[:,:,0], vestfe_valid_ar), axis=1)\n","\n","  vestfe_test = vest_model.transform(Xtest[:,:,0])\n","  vestfe_test_ar = vestfe_test.values\n","  X_ts_augmented = np.concatenate((Xtest[:,:,0], vestfe_test_ar), axis=1)\n","\n","  Xtrain_arrayfe = X_tr_augmented.reshape((X_tr_augmented.shape[0], X_tr_augmented.shape[1], 1))\n","  Xvalid_arrayfe = X_vl_augmented.reshape((X_vl_augmented.shape[0], X_vl_augmented.shape[1], 1))\n","  Xtest = X_ts_augmented.reshape((X_ts_augmented.shape[0], X_ts_augmented.shape[1], 1))\n","\n","  # Input RF and MSVR:\n","  X_train = Xtrain_arrayfe.reshape((Xtrain_arrayfe.shape[0],-1))\n","  X_valid = Xvalid_arrayfe.reshape((Xvalid_arrayfe.shape[0],-1))\n","  X_test = Xtest.reshape((Xtest.shape[0],-1))\n","\n","  # Vest feature shapes for Seq2Seq context\n","  Xtrain_arrayfe_enc = Xtrain_arrayfe[:,0:24,:]\n","  Xvalid_arrayfe_enc = Xvalid_arrayfe[:,0:24,:]\n","  Xtest_enc = Xtest[:,0:24,:]\n","  \n","  Xtrain_arrayfe_dec = Xtrain_arrayfe[:,24:,:]\n","  Xvalid_arrayfe_dec = Xvalid_arrayfe[:,24:,:]\n","  Xtest_dec = Xtest[:,24:,:]\n","\n","else:\n","  pass"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1650666596494,"user":{"displayName":"Antonia Deeplearning","userId":"03469055858719879397"},"user_tz":-120},"id":"UQ-PpBtQNMVp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"89d53057-1d2c-406b-ba41-9bba7345dd31"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of train and val array: (7446, 24, 1), (1860, 24, 1)\n","Shape of train and val y array: (7446, 24), (1860, 24)\n","Shape of test data X-array and y-array: (112, 24, 1), (112, 24)\n","(7446, 24)\n","(1860, 24)\n","(112, 24)\n"]}],"source":["print(f'Shape of train and val array: {Xtrain_arrayfe.shape}, {Xvalid_arrayfe.shape}')\n","print(f'Shape of train and val y array: {ytrain_arrayfe.shape}, {yvalid_arrayfe.shape}')\n","print(f'Shape of test data X-array and y-array: {Xtest.shape}, {ytest.shape}') \n","print(X_train.shape)\n","print(X_valid.shape)\n","print(X_test.shape)\n","# assertion test einfügen dass error gibt wenn falsch"]},{"cell_type":"markdown","metadata":{"id":"jfjbmAIeQVo1"},"source":["##### Taken Transformation"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1650666596495,"user":{"displayName":"Antonia Deeplearning","userId":"03469055858719879397"},"user_tz":-120},"id":"jDUm68y8loY6","colab":{"base_uri":"https://localhost:8080/"},"outputId":"4c4a7938-afb4-4191-bda2-97b2d167af19"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of train and val array: (7446, 24, 1), (1860, 24, 1)\n","Shape of train and val y array: (7446, 24), (1860, 24)\n","Shape of test data X-array and y-array: (112, 24, 1), (112, 24)\n","(7446, 24)\n","(1860, 24)\n","(112, 24)\n"]}],"source":["if 'taken_features' in locals():\n","\n","  TE = TakensEmbedding(time_delay=1, dimension=3, flatten=True)\n","\n","  # One dimensional case\n","  if dimension_var == 'one':\n","    Xtrain_arrayfe = TE.fit_transform(Xtrain_arrayfe.reshape((Xtrain_arrayfe.shape[0], Xtrain_arrayfe.shape[1])))\n","    Xvalid_arrayfe = TE.transform(Xvalid_arrayfe.reshape((Xvalid_arrayfe.shape[0], Xvalid_arrayfe.shape[1])))\n","    Xtest = TE.transform(Xtest.reshape((Xtest.shape[0], Xtest.shape[1])))\n","    print(f'Shapes of train, valid and test input: {Xtrain_arrayfe.shape}, {Xvalid_arrayfe.shape}, {Xtest.shape}')\n","  else:\n","    Xtrain_arrayfe = TE.fit_transform(np.transpose(Xtrain_arrayfe, (0, 2, 1)))\n","    Xvalid_arrayfe = TE.transform(np.transpose(Xvalid_arrayfe, (0, 2, 1)))\n","    Xtest = TE.transform(np.transpose(Xtest, (0, 2, 1)))\n","    print(f'Shapes of train, valid and test input: {Xtrain_arrayfe.shape}, {Xvalid_arrayfe.shape}, {Xtest.shape}')  \n","  # For RF and MSVR\n","  X_train = Xtrain_arrayfe.reshape((Xtrain_arrayfe.shape[0], -1))\n","  X_test = Xtest.reshape((Xtest.shape[0], -1))\n","\n","print(f'Shape of train and val array: {Xtrain_arrayfe.shape}, {Xvalid_arrayfe.shape}')\n","print(f'Shape of train and val y array: {ytrain_arrayfe.shape}, {yvalid_arrayfe.shape}')\n","print(f'Shape of test data X-array and y-array: {Xtest.shape}, {ytest.shape}') \n","print(X_train.shape)\n","print(X_valid.shape)\n","print(X_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"igmq_wA8_Wmd"},"source":["### Model Training\n"]},{"cell_type":"markdown","metadata":{"id":"5fHdCVJNGSnf"},"source":["#### XGBOOST"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":124529,"status":"ok","timestamp":1650666721016,"user":{"displayName":"Antonia Deeplearning","userId":"03469055858719879397"},"user_tz":-120},"id":"vgBQbdxso8cy","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ae1f3a60-d67e-448f-dcd0-0e292748a9e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting Model: 0\n","Starting Model: 1\n","Starting Model: 2\n","Starting Model: 3\n","Starting Model: 4\n","Starting Model: 5\n","Starting Model: 6\n","Starting Model: 7\n","Starting Model: 8\n","Starting Model: 9\n","Starting Model: 10\n","Starting Model: 11\n","Starting Model: 12\n","Starting Model: 13\n","Starting Model: 14\n","Starting Model: 15\n","Starting Model: 16\n","Starting Model: 17\n","Starting Model: 18\n","Starting Model: 19\n","Starting Model: 20\n","Starting Model: 21\n","Starting Model: 22\n","Starting Model: 23\n","GPU Training Time: 124.41970920562744 seconds\n"]}],"source":["tmp = time.time()\n","results_lst = []\n","for i in range(24): #ytrain_arrayfe.shape[1]\n","  # Convert input data from numpy to XGBoost format\n","  print(f'Starting Model: {i}')\n","  dtrain = xgb.DMatrix(X_train, label=ytrain_arrayfe[:,i])\n","  dvalid = xgb.DMatrix(X_valid, label=yvalid_arrayfe[:,i])\n","  dtest = xgb.DMatrix(X_test, label=ytest[:,i])\n","\n","  # specify validations set to watch performance\n","  watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n","  \n","  # Train model\n","  xgbtrained = xgb.train(param, dtrain, num_round, watchlist, verbose_eval=False) #\n","\n","  #save model\n","  joblib.dump(xgbtrained, f'{data_url}xgb_model.dat')\n","  xgbtrained.__del__()\n","\n","  #load saved model\n","  xgbtrained = joblib.load(f'{data_url}xgb_model.dat')\n","\n","  predictions = xgbtrained.predict(dtest)\n","  results_lst.append(predictions.reshape((predictions.shape[0], 1)))\n","  \n","xgb_train_time = time.time() - tmp\n","print(\"GPU Training Time: %s seconds\" % (str(xgb_train_time)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1650666721017,"user":{"displayName":"Antonia Deeplearning","userId":"03469055858719879397"},"user_tz":-120},"id":"oBQpo-6FxJcb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ccc2a3d-9801-40c9-c43b-c33a3f3d55fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","17.64,14.45,0.91,0.86, 0.17, 0.80\n"]}],"source":["y_pred = np.hstack(results_lst)\n","y_pred_fridge = y_pred.reshape((-1,1))\n","y_pred_fridge = preproc_target.inverse_transform(y_pred_fridge)\n","\n","y_pred_wind = y_pred_fridge.reshape(df_test_true_wind.shape)\n","\n","evaluation_dict['XGBoost'] = evaluate_predictions_flat(df_test_true, y_pred_fridge, test_y = Xtest_fridge, testing=False)\n","evaluation_dict['XGBoost'].append(xgb_train_time)\n"]},{"cell_type":"markdown","metadata":{"id":"5ohFEoLCzHoe"},"source":["#### MSVR"]},{"cell_type":"markdown","metadata":{"id":"ZcUOBBlWc9yX"},"source":["##### Code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"704HgzHdY3Ta"},"outputs":[],"source":["class MSVR():\n","    def __init__(self, kernel='rbf', degree=3, gamma=None, coef0=0.0, tol=0.001, C=1.0, epsilon=0.1):\n","        super(MSVR, self).__init__()\n","        self.kernel = kernel\n","        self.degree = degree\n","        self.gamma = gamma\n","        self.coef0 = coef0\n","        self.tol = tol\n","        self.C = C\n","        self.epsilon = epsilon\n","        self.Beta = None\n","        self.NSV = None\n","        self.xTrain = None\n","\n","    def fit(self, x, y):\n","        self.xTrain = x.copy()\n","        C = self.C\n","        epsi = self.epsilon\n","        tol = self.tol\n","\n","        n_m = np.shape(x)[0]  # num of samples\n","        n_d = np.shape(x)[1]  # input data dimensionality\n","        n_k = np.shape(y)[1]  # output data dimensionality (output variables)\n","\n","        # H = kernelmatrix(ker, x, x, par)\n","        H = pairwise_kernels(x, x, metric=self.kernel, filter_params=True,\n","                             degree=self.degree, gamma=self.gamma, coef0=self.coef0)\n","\n","        self.Beta = np.zeros((n_m, n_k))\n","\n","        #E = prediction error per output (n_m * n_k)\n","        E = y - np.dot(H, self.Beta)\n","        #RSE\n","        u = np.sqrt(np.sum(E**2, 1, keepdims=True))\n","\n","        #RMSE\n","        RMSE = []\n","        RMSE_0 = np.sqrt(np.mean(u**2))\n","        RMSE.append(RMSE_0)\n","\n","        #points for which prediction error is larger than epsilon\n","        i1 = np.where(u > epsi)[0]\n","\n","        #set initial values of alphas a (n_m * 1)\n","        a = 2 * C * (u - epsi) / u\n","\n","        #L (n_m * 1)\n","        L = np.zeros(u.shape)\n","\n","        # we modify only entries for which  u > epsi. with the sq slack\n","        L[i1] = u[i1]**2 - 2 * epsi * u[i1] + epsi**2\n","\n","        #Lp is the quantity to minimize (sq norm of parameters + slacks)\n","        Lp = []\n","        BetaH = np.dot(np.dot(self.Beta.T, H), self.Beta)\n","        Lp_0 = np.sum(np.diag(BetaH), 0) / 2 + C * np.sum(L)/2\n","        Lp.append(Lp_0)\n","\n","        eta = 1\n","        k = 1\n","        hacer = 1\n","        val = 1\n","\n","        while(hacer):\n","            Beta_a = self.Beta.copy()\n","            E_a = E.copy()\n","            u_a = u.copy()\n","            i1_a = i1.copy()\n","\n","            M1 = H[i1][:, i1] + \\\n","                np.diagflat(1/a[i1]) + 1e-10 * np.eye(len(a[i1]))\n","\n","            #compute betas\n","            #       sal1 = np.dot(np.linalg.pinv(M1),y[i1])  #求逆or广义逆（M-P逆）无法保证M1一定是可逆的？\n","            sal1 = np.dot(np.linalg.inv(M1), y[i1])\n","\n","            eta = 1\n","            self.Beta = np.zeros(self.Beta.shape)\n","            self.Beta[i1] = sal1.copy()\n","\n","            #error\n","            E = y - np.dot(H, self.Beta)\n","            #RSE\n","            u = np.sqrt(np.sum(E**2, 1)).reshape(n_m, 1)\n","            i1 = np.where(u >= epsi)[0]\n","\n","            L = np.zeros(u.shape)\n","            L[i1] = u[i1]**2 - 2 * epsi * u[i1] + epsi**2\n","\n","            #%recompute the loss function\n","            BetaH = np.dot(np.dot(self.Beta.T, H), self.Beta)\n","            Lp_k = np.sum(np.diag(BetaH), 0) / 2 + C * np.sum(L)/2\n","            Lp.append(Lp_k)\n","\n","            #Loop where we keep alphas and modify betas\n","            while(Lp[k] > Lp[k-1]):\n","                eta = eta/10\n","                i1 = i1_a.copy()\n","\n","                self.Beta = np.zeros(self.Beta.shape)\n","                #%the new betas are a combination of the current (sal1)\n","                #and of the previous iteration (Beta_a)\n","                self.Beta[i1] = eta*sal1 + (1-eta)*Beta_a[i1]\n","\n","                E = y - np.dot(H, self.Beta)\n","                u = np.sqrt(np.sum(E**2, 1)).reshape(n_m, 1)\n","\n","                i1 = np.where(u >= epsi)[0]\n","\n","                L = np.zeros(u.shape)\n","                L[i1] = u[i1]**2 - 2 * epsi * u[i1] + epsi**2\n","                BetaH = np.dot(np.dot(self.Beta.T, H), self.Beta)\n","                Lp_k = np.sum(np.diag(BetaH), 0) / 2 + C * np.sum(L)/2\n","                Lp[k] = Lp_k\n","\n","                #stopping criterion 1\n","                if(eta < 1e-16):\n","                    Lp[k] = Lp[k-1] - 1e-15\n","                    self.Beta = Beta_a.copy()\n","\n","                    u = u_a.copy()\n","                    i1 = i1_a.copy()\n","\n","                    hacer = 0\n","\n","            #here we modify the alphas and keep betas\n","            a_a = a.copy()\n","            a = 2 * C * (u - epsi) / u\n","\n","            RMSE_k = np.sqrt(np.mean(u**2))\n","            RMSE.append(RMSE_k)\n","\n","            if((Lp[k-1]-Lp[k])/Lp[k-1] < tol):\n","                hacer = 0\n","\n","            k = k + 1\n","\n","            #stopping criterion #algorithm does not converge. (val = -1)\n","            if(len(i1) == 0):\n","                hacer = 0\n","                self.Beta = np.zeros(self.Beta.shape)\n","                val = -1\n","\n","        self.NSV = len(i1)\n","\n","    def predict(self, x):\n","        H = pairwise_kernels(x, self.xTrain, metric=self.kernel, filter_params=True,\n","                             degree=self.degree, gamma=self.gamma, coef0=self.coef0)\n","        yPred = np.dot(H, self.Beta)\n","        return yPred\n","\n","    # def score(self,x):"]},{"cell_type":"markdown","metadata":{"id":"P0GtueHodCS-"},"source":["##### Predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Xc44iYMbL5i"},"outputs":[],"source":["# define model\n","start = time.time()\n","# Train Model 10 Fold:\n","msvr_result_lst = []\n","for i in [42, 10, 567, 239, 400, 1390, 380, 9, 27, 769]:\n","  \n","  # Training loop\n","  seed_value = i\n","  os.environ['PYTHONHASHSEED']=str(seed_value)\n","  random.seed(seed_value)\n","  np.random.seed(seed_value)\n","  msvr = MSVR(kernel = msvr_kernel, gamma = get_msvr_gamma(X_train, relational=msvr_relational, factor=msvr_factor), epsilon= msvr_epsilon, degree= msvr_degree, C= msvr_c) \n","  msvr.fit(X_train, ytrain_arrayfe)\n","  y_pred = msvr.predict(X_test)\n","  msvr_result_lst.append(y_pred) \n","end = time.time()\n","\n","msvr_train_time = end - start"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1650667322439,"user":{"displayName":"Antonia Deeplearning","userId":"03469055858719879397"},"user_tz":-120},"id":"_suqQFebZrDx","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1296b7d5-e54d-4e4b-90a7-57d88668b641"},"outputs":[{"output_type":"stream","name":"stdout","text":["RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","15.72,13.52,0.85,0.80, 0.16, 0.78\n","RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","15.72,13.52,0.85,0.80, 0.16, 0.78\n","RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","15.72,13.52,0.85,0.80, 0.16, 0.78\n","RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","15.72,13.52,0.85,0.80, 0.16, 0.78\n","RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","15.72,13.52,0.85,0.80, 0.16, 0.78\n","RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","15.72,13.52,0.85,0.80, 0.16, 0.78\n","RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","15.72,13.52,0.85,0.80, 0.16, 0.78\n","RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","15.72,13.52,0.85,0.80, 0.16, 0.78\n","RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","15.72,13.52,0.85,0.80, 0.16, 0.78\n","RMSE/MAE/wMAPE/MASE/NRMSE/ACCURACY\n","15.72,13.52,0.85,0.80, 0.16, 0.78\n"]}],"source":["rmse_lst_msvr, mae_lst_msvr, wmape_lst_msvr, mase_lst_msvr, nrmse_lst_msvr, acc_lst_msvr, acc_90_lst_msvr = print_originalvalues_results(msvr_result_lst, df_test_true, Xtest_fridge)\n","\n","msvr_rmse = sum(rmse_lst_msvr)/len(rmse_lst_msvr)\n","msvr_mae = sum(mae_lst_msvr)/len(mae_lst_msvr)\n","msvr_wmape = sum(wmape_lst_msvr)/len(wmape_lst_msvr)\n","msvr_mase = sum(mase_lst_msvr)/len(mase_lst_msvr)\n","msvr_nrmse = sum(nrmse_lst_msvr)/len(nrmse_lst_msvr)\n","msvr_acc = sum(acc_lst_msvr)/len(acc_lst_msvr)\n","msvr_acc_90 = sum(acc_90_lst_msvr)/len(acc_90_lst_msvr)\n","\n","msvr_lst_results = [msvr_rmse, msvr_mae, msvr_wmape, msvr_mase, msvr_nrmse, msvr_acc, msvr_acc_90, msvr_train_time]\n","evaluation_dict['MSVR'] = msvr_lst_results"]},{"cell_type":"markdown","metadata":{"id":"Ss454fqYzEro"},"source":["#### LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0kKAv3v02ZBV"},"outputs":[],"source":["start = time.time()\n","# Train Model 10 Fold:\n","lstm_result_lst = []\n","for i in [42, 10, 567, 239, 400, 1390, 380, 9, 27, 769]:\n","  # Define input layers\n","  sequential_input = Input(Xtrain_arrayfe[0].shape)\n","  \n","  # Model Architecture: LSTM\n","  lstm1 = LSTM(hidd_dim, return_sequences=True, input_shape=Xtrain_arrayfe.shape)(sequential_input)\n","  lr1 = LeakyReLU(alpha=lr_alpha)(lstm1)\n","  drpt1 = Dropout(drpt_rate)(lr1)\n","  lstm2 = LSTM(hidd_dim)(drpt1)\n","  lr2 = LeakyReLU(alpha=lr_alpha)(lstm2)\n","  drpt2 = Dropout(drpt_rate)(lr2)\n","\n","  dense = Dense(24)(drpt2)\n","  outputs = LeakyReLU(alpha=lr_alpha)(dense)\n","  \n","  # Define the model\n","  lstm = Model(inputs=sequential_input, outputs=outputs)\n","  lstm.compile(loss=loss, optimizer=optimizer)\n","  #lstm.compile(loss = [loss1,loss2], loss_weights = [l1,l2], optimizer=optimizer)\n","  \n","  # Training loop\n","  seed_value = i\n","  os.environ['PYTHONHASHSEED']=str(seed_value)\n","  random.seed(seed_value)\n","  np.random.seed(seed_value)\n","  tf.compat.v1.set_random_seed(seed_value)\n","  lstm.fit(Xtrain_arrayfe, ytrain_arrayfe, validation_data = (Xvalid_arrayfe, yvalid_arrayfe), epochs = epochs)\n","  y_pred = lstm.predict(Xtest)\n","  lstm_result_lst.append(y_pred)\n","  lstm.reset_states() \n","end = time.time()\n","\n","print(\"Time elapsed:\", end - start)\n","\n","lstm_train_time = end - start"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nnd-fq4H42b1"},"outputs":[],"source":["rmse_lst, mae_lst, wmape_lst, mase_lst, nrmse_lst, acc_lst, acc_lst_90 = print_originalvalues_results(lstm_result_lst, df_test_true, Xtest_fridge)\n","\n","lstm_rmse = sum(rmse_lst)/len(rmse_lst)\n","lstm_mae = sum(mae_lst)/len(mae_lst)\n","lstm_wmape = sum(wmape_lst)/len(wmape_lst)\n","lstm_mase = sum(mase_lst)/len(mase_lst)\n","lstm_nrmse = sum(nrmse_lst)/len(nrmse_lst)\n","lstm_acc = sum(acc_lst)/len(acc_lst)\n","lstm_acc_90 = sum(acc_lst_90)/len(acc_lst_90)\n","\n","print('Mean RMSE, MAE, WMAPE, MASE, NRMSE, ACCURACY:')\n","print(f'{lstm_rmse:.5f}/{lstm_mae:.5f}/{lstm_wmape:.5f}/{lstm_mase:.5f}/{lstm_nrmse:.5f}/{lstm_acc:.5f}')\n","\n","lstm_lst_results = [lstm_rmse, lstm_mae, lstm_wmape, lstm_mase, lstm_nrmse, lstm_acc, lstm_acc_90, lstm_train_time]\n","evaluation_dict['LSTM'] = lstm_lst_results"]},{"cell_type":"markdown","metadata":{"id":"4Yhu8nqrGA54"},"source":["#### BiDiLSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l6D9YNU8c1mP"},"outputs":[],"source":["start = time.time()\n","# Train Model 10 Fold:\n","bidilstm_result_lst = []\n","for i in [42, 10, 567, 239, 400, 1390, 380, 9, 27, 769]:\n","\n","  # Define input layers\n","  sequential_input = Input(Xtrain_arrayfe[0].shape)\n","  \n","  # Model Architecture: LSTM\n","  bidilstm1 = Bidirectional(LSTM(hidd_dim))(sequential_input) #, return_sequences=True\n","  lr1 = LeakyReLU(alpha=lr_alpha)(bidilstm1)\n","  drpt1 = Dropout(drpt_rate)(lr1)\n","\n","  outputs = Dense(24)(drpt1) #, activation='sigmoid'\n","  outputs = LeakyReLU(alpha=lr_alpha)(outputs)\n","  \n","  # Define the model\n","  bidilstm = Model(inputs=sequential_input, outputs=outputs)\n","  bidilstm.compile(loss=loss, optimizer=optimizer)\n","\n","\n","  seed_value = i\n","  os.environ['PYTHONHASHSEED']=str(seed_value)\n","  random.seed(seed_value)\n","  np.random.seed(seed_value)\n","  tf.compat.v1.set_random_seed(seed_value)\n","  bidilstm.fit(Xtrain_arrayfe, ytrain_arrayfe, validation_data = (Xvalid_arrayfe, yvalid_arrayfe), epochs = epochs)\n","  y_pred = bidilstm.predict(Xtest)\n","  bidilstm_result_lst.append(y_pred)\n","end = time.time()\n","\n","print(\"Time elapsed:\", end - start)\n","# save the lastm Model (save best model? + seed?)\n","#bidilstm.save(f'{model_path}{model_name}')\n","bilstm_train_time = end - start"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M9rsZDN_ku6s"},"outputs":[],"source":["# calculate differences to original data for many models:\n","rmse_lst, mae_lst, wmape_lst, mase_lst, nrmse_lst, acc_lst, acc_lst_90 = print_originalvalues_results(bidilstm_result_lst, df_test_true, Xtest_fridge)\n","\n","bilstm_rmse = sum(rmse_lst)/len(rmse_lst)\n","bilstm_mae = sum(mae_lst)/len(mae_lst)\n","bilstm_wmape = sum(wmape_lst)/len(wmape_lst)\n","bilstm_mase = sum(mase_lst)/len(mase_lst)\n","bilstm_nrmse = sum(nrmse_lst)/len(nrmse_lst)\n","bilstm_acc = sum(acc_lst)/len(acc_lst)\n","bilstm_acc_90 = sum(acc_lst_90)/len(acc_lst_90)\n","\n","print('Mean RMSE, MAE, WMAPE, MASE, NRMSE, ACCURACY:')\n","print(f'{bilstm_rmse:.5f}/{bilstm_mae:.5f}/{bilstm_wmape:.5f}/{bilstm_mase:.5f}/{bilstm_nrmse:.5f}/{bilstm_acc:.5f}')\n","\n","y_pred_wind_lst_bidilstm = []\n","for i in range(len(bidilstm_result_lst)):\n","  y_pred_wind = preproc_target.inverse_transform(bidilstm_result_lst[i].reshape((-1,1)))\n","  y_pred_wind = y_pred_wind.reshape(df_test_true_wind.shape)\n","  y_pred_wind_lst_bidilstm.append(y_pred_wind)\n","  rmse_wind, mae_wind, wmape, mase_wind, nrmse_wind, acc_wind, acc_wind_90 = evaluate_predictions_windows(df_test_true_wind, y_pred_wind, test_y = Xtest_fridge_wind, testing=False)\n","\n","bilstm_lst_results = [bilstm_rmse, bilstm_mae, bilstm_wmape, bilstm_mase, bilstm_nrmse, bilstm_acc, bilstm_acc_90, bilstm_train_time]\n","evaluation_dict['BiLSTM'] = bilstm_lst_results"]},{"cell_type":"markdown","metadata":{"id":"wErGc8pZGK8G"},"source":["#### Seq2Seq"]},{"cell_type":"markdown","metadata":{"id":"-fFZ-DLLkkfu"},"source":["##### Seq2Seq from Razghandi et al. 2021"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MhRz40WmGKcU"},"outputs":[],"source":["# decoder output:\n","Xtrain_arrayfe_flip = np.flip(Xtrain_arrayfe, axis=1)[:,:,0]\n","Xvalid_arrayfe_flip = np.flip(Xvalid_arrayfe, axis=1)[:,:,0]\n","Xtest_flip = np.flip(Xtest, axis=1)[:,:,0]\n","\n","optimizer1 = tf.keras.optimizers.Adam(lr=0.001)\n","\n","start = time.time()\n","# Train Model 10 Fold:\n","seq2seq_result_lst = []\n","for i in [42, 10, 567, 239, 400, 1390, 380, 9, 27, 769]:\n","  # Use model in the Razghandi2021 paper Encoder Decoder\n","  # try with decoder return sequence and not return sequence\n","  encoder_input = tf.keras.Input(\n","      shape=Xtrain_arrayfe[0].shape, name='encoder_input') \n","  \n","  encoder = tf.keras.layers.LSTM(hidd_dim, return_state=True, return_sequences=True)\n","  encoder_outputs, state_h, state_c = encoder(encoder_input)\n","  \n","  decoder_lstm = tf.keras.layers.LSTM(hidd_dim, return_sequences=True) #\n","  x = decoder_lstm(encoder_outputs,initial_state=[state_h, state_c])\n","  \n","  x = tf.keras.layers.LSTM(Xtrain_arrayfe.shape[1])(x)\n","  output = Dense(Xtrain_arrayfe.shape[1])(x)\n","  \n","  encdec = tf.keras.models.Model(inputs=encoder_input, outputs=output)\n","  encdec.compile(loss=loss, optimizer=optimizer1, metrics=[\"mse\"])\n","\n","  # Training Loop\n","  seed_value = i\n","  os.environ['PYTHONHASHSEED']=str(seed_value)\n","  random.seed(seed_value)\n","  np.random.seed(seed_value)\n","  tf.compat.v1.set_random_seed(seed_value)\n","  encdec.fit(Xtrain_arrayfe, y=Xtrain_arrayfe_flip, validation_data=(Xvalid_arrayfe, Xvalid_arrayfe_flip), epochs = 100)\n","\n","  generator_input = tf.keras.Input(\n","      shape=Xtrain_arrayfe[0].shape, name='generator_input') \n","  \n","  encoder.trainable=False\n","  encoder_gen, state_h, state_c = encoder(generator_input)\n","  \n","  generator_lstm = tf.keras.layers.LSTM(hidd_dim) #\n","  x = generator_lstm(encoder_gen,initial_state=[state_h, state_c])\n","  \n","  output = tf.keras.layers.Dense(24)(x)\n","  \n","  seq2seq = tf.keras.models.Model(inputs=generator_input, outputs=output)\n","  seq2seq.compile(loss=loss, optimizer=optimizer, metrics=[\"mse\"])\n","  seq2seq.fit(Xtrain_arrayfe, y=ytrain_arrayfe, validation_data=(Xvalid_arrayfe, yvalid_arrayfe), epochs = epochs)\n","\n","  y_pred = seq2seq.predict(Xtest)\n","  seq2seq_result_lst.append(y_pred)\n","end = time.time()\n","\n","print(\"Time elapsed:\", end - start)\n","\n","s2sra_train_time = end - start"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2XsR8QUckXsf"},"outputs":[],"source":["# calculate differences to original data for many models:\n","rmse_lst, mae_lst, wmape_lst, mase_lst, nrmse_lst, acc_lst, acc_lst_90 = print_originalvalues_results(seq2seq_result_lst, df_test_true, Xtest_fridge)\n","\n","s2sra_rmse = sum(rmse_lst)/len(rmse_lst)\n","s2sra_mae = sum(mae_lst)/len(mae_lst)\n","s2sra_wmape = sum(wmape_lst)/len(wmape_lst)\n","s2sra_mase = sum(mase_lst)/len(mase_lst)\n","s2sra_nrmse = sum(nrmse_lst)/len(nrmse_lst)\n","s2sra_acc = sum(acc_lst)/len(acc_lst)\n","s2sra_acc_90 = sum(acc_lst_90)/len(acc_lst_90)\n","\n","print('Mean RMSE, MAE, WMAPE, MASE, NRMSE, ACCURACY:')\n","print(f'{s2sra_rmse:.5f}/{s2sra_mae:.5f}/{s2sra_wmape:.5f}/{s2sra_mase:.5f}/{s2sra_nrmse:.5f}/{s2sra_acc:.5f}')\n","\n","#y_pred_wind_lst_seq2seq = []\n","#for i in range(len(seq2seq_result_lst)):\n","#  y_pred_wind = preproc_target.inverse_transform(seq2seq_result_lst[i].reshape((-1,1)))\n","#  y_pred_wind = y_pred_wind.reshape(df_test_true_wind.shape)\n","#  y_pred_wind_lst_seq2seq.append(y_pred_wind)\n","#  rmse_wind, mae_wind, wmape, mase_wind, nrmse_wind, acc_wind, acc_wind_90 = evaluate_predictions_windows(df_test_true_wind, y_pred_wind, test_y = Xtest_fridge_wind, testing=False)\n","\n","s2sra_lst_results = [s2sra_rmse, s2sra_mae, s2sra_wmape, s2sra_mase, s2sra_nrmse, s2sra_acc, s2sra_acc_90, s2sra_train_time]\n","evaluation_dict['S2S Reversed'] = s2sra_lst_results"]},{"cell_type":"markdown","metadata":{"id":"MdpYMGImIEA6"},"source":["##### Seq2Seq with Context Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1DnmfsaIH_G5"},"outputs":[],"source":["start = time.time()\n","# Train Model 10 Fold:\n","seq2seq_extra_result_lst = []\n","for i in [42, 10, 567, 239, 400, 1390, 380, 9, 27, 769]:\n","\n","  # Use model in the Razghandi2021 paper Encoder Decoder\n","  # try with decoder return sequence and not return sequence\n","  encoder_input = tf.keras.Input(\n","      shape=Xtrain_arrayfe_enc[0].shape, name='encoder_input') \n","  \n","  encoder = tf.keras.layers.LSTM(hidd_dim_context, return_state=True, return_sequences=True)\n","  encoder_outputs, state_h, state_c = encoder(encoder_input)\n","  \n","  decoder_inputs = tf.keras.Input(\n","      shape=Xtrain_arrayfe_dec[0].shape, name='decoder_inputs')\n","  \n","  decoder_lstm = tf.keras.layers.LSTM(hidd_dim_context, return_sequences=True) #\n","  x = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n","  \n","  x = tf.keras.layers.LSTM(Xtrain_arrayfe.shape[1])(x)\n","  output = Dense(24)(x)\n","  \n","  seq2seq_extra = tf.keras.models.Model(inputs=[encoder_input, decoder_inputs], outputs=output)\n","  seq2seq_extra.compile(loss=loss, optimizer=optimizer, metrics=[\"mse\"])\n","  \n","  # Training Loop\n","  seed_value = i\n","  os.environ['PYTHONHASHSEED']=str(seed_value)\n","  random.seed(seed_value)\n","  np.random.seed(seed_value)\n","  tf.compat.v1.set_random_seed(seed_value)\n","  seq2seq_extra.fit((Xtrain_arrayfe_enc,Xtrain_arrayfe_dec), y=ytrain_arrayfe, validation_data=((Xvalid_arrayfe_enc,Xvalid_arrayfe_dec), yvalid_arrayfe), epochs = epochs)\n","  y_pred = seq2seq_extra.predict((Xtest_enc, Xtest_dec))\n","  seq2seq_extra_result_lst.append(y_pred)\n","end = time.time()\n","\n","print(\"Time elapsed:\", end - start)\n","# save the lastm Model (save best model? + seed?)\n","#seq2seq.save(f'{model_path}{model_name}')\n","s2sex_train_time = end - start"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EPs61_6RZtM"},"outputs":[],"source":["# calculate differences to original data for many models:\n","rmse_lst, mae_lst, wmape_lst, mase_lst, nrmse_lst, acc_lst, acc_lst_90 = print_originalvalues_results(seq2seq_extra_result_lst, df_test_true, Xtest_fridge)\n","\n","s2sex_rmse = sum(rmse_lst)/len(rmse_lst)\n","s2sex_mae = sum(mae_lst)/len(mae_lst)\n","s2sex_wmape = sum(wmape_lst)/len(wmape_lst)\n","s2sex_mase = sum(mase_lst)/len(mase_lst)\n","s2sex_nrmse = sum(nrmse_lst)/len(nrmse_lst)\n","s2sex_acc = sum(acc_lst)/len(acc_lst)\n","s2sex_acc_90 = sum(acc_lst_90)/len(acc_lst_90)\n","\n","print('Mean RMSE, MAE, WMAPE, MASE, NRMSE, ACCURACY:')\n","print(f'{s2sex_rmse:.5f}/{s2sex_mae:.5f}/{s2sex_wmape:.5f}/{s2sex_mase:.5f}/{s2sex_nrmse:.5f}/{s2sex_acc:.5f}')\n","\n","#y_pred_wind_lst_seq2seq_ae = []\n","#for i in range(len(seq2seq_extra_result_lst)):\n","#  y_pred_wind = preproc_target.inverse_transform(seq2seq_extra_result_lst[i].reshape((-1,1)))\n","#  y_pred_wind = y_pred_wind.reshape(df_test_true_wind.shape)\n","#  y_pred_wind_lst_seq2seq_ae.append(y_pred_wind)\n","#  rmse_wind, mae_wind, wmape, mase_wind, nrmse_wind, acc_wind, acc_wind_90 = evaluate_predictions_windows(df_test_true_wind, y_pred_wind, test_y = Xtest_fridge_wind, testing=False)\n","\n","s2sex_lst_results = [s2sex_rmse, s2sex_mae, s2sex_wmape, s2sex_mase, s2sex_nrmse, s2sex_acc, s2sex_acc_90, s2sex_train_time]\n","evaluation_dict['S2S Context'] = s2sex_lst_results"]},{"cell_type":"markdown","metadata":{"id":"IDtbJvKNGfnx"},"source":["#### FFNN\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8dBISb8ieMme"},"outputs":[],"source":["start = time.time()\n","# Train Model 10 Fold:\n","ffnn_result_lst = []\n","for i in [42, 10, 567, 239, 400, 1390, 380, 9, 27, 769]:\n","\n","  # Define input layers\n","  sequential_input = Input(X_train[0].shape)\n","  \n","  ffnn1 = Dense(hidd_dim_ffnn)(sequential_input)\n","  lr1 = LeakyReLU(alpha=lr_alpha)(ffnn1)\n","  drpt1 = Dropout(drpt_rate)(lr1)\n","  ffnn2 = Dense(hidd_dim_ffnn)(drpt1)\n","  lr2 = LeakyReLU(alpha=lr_alpha)(ffnn2)\n","  drpt2 = Dropout(drpt_rate)(lr2)\n","  ffnn3 = Dense(hidd_dim_ffnn)(drpt2)\n","  lr3 = LeakyReLU(alpha=lr_alpha)(ffnn3)\n","  drpt3 = Dropout(drpt_rate)(lr3)\n","  ffnn4 = Dense(hidd_dim_ffnn)(drpt3)\n","  lr4 = LeakyReLU(alpha=lr_alpha)(ffnn4)\n","  drpt4 = Dropout(drpt_rate)(lr4)\n","  ffnn5 = Dense(hidd_dim_ffnn)(drpt4)\n","  lr5 = LeakyReLU(alpha=lr_alpha)(ffnn5)\n","  drpt5 = Dropout(drpt_rate)(lr5)\n","  outputs = Dense(24)(drpt5)  \n","  # Define the model\n","  ffnn = Model(inputs=sequential_input, outputs=outputs)\n","  ffnn.compile(loss=loss, optimizer=optimizer)\n","\n","\n","  seed_value = i\n","  os.environ['PYTHONHASHSEED']=str(seed_value)\n","  random.seed(seed_value)\n","  np.random.seed(seed_value)\n","  tf.compat.v1.set_random_seed(seed_value)\n","  ffnn.fit(X_train, ytrain_arrayfe, validation_data = (X_valid, yvalid_arrayfe), epochs = epochs, batch_size=batch_size)\n","  y_pred = ffnn.predict(X_test)\n","  ffnn_result_lst.append(y_pred)\n","\n","end = time.time()\n","\n","print(\"Time elapsed:\", end - start)\n","\n","ffnn_train_time = end - start"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B6_-wfQ0TiKe"},"outputs":[],"source":["# calculate differences to original data for many models:\n","rmse_lst, mae_lst, wmape_lst, mase_lst, nrmse_lst, acc_lst, acc_lst_90 = print_originalvalues_results(ffnn_result_lst, df_test_true, Xtest_fridge)\n","\n","ffnn_rmse = sum(rmse_lst)/len(rmse_lst)\n","ffnn_mae = sum(mae_lst)/len(mae_lst)\n","ffnn_wmape = sum(wmape_lst)/len(wmape_lst)\n","ffnn_mase = sum(mase_lst)/len(mase_lst)\n","ffnn_nrmse = sum(nrmse_lst)/len(nrmse_lst)\n","ffnn_acc = sum(acc_lst)/len(acc_lst)\n","ffnn_acc_90 = sum(acc_lst_90)/len(acc_lst_90)\n","\n","print('Mean RMSE, MAE, WMAPE, MASE, NRMSE, ACCURACY:')\n","print(f'{ffnn_rmse:.5f}/{ffnn_mae:.5f}/{ffnn_wmape:.5f}/{ffnn_mase:.5f}/{ffnn_nrmse:.5f}/{ffnn_acc:.5f}')\n","\n","#y_pred_wind_lst_ffnn = []\n","#for i in range(len(ffnn_result_lst)):\n","#  y_pred_wind = preproc_target.inverse_transform(ffnn_result_lst[i].reshape((-1,1)))\n","#  y_pred_wind = y_pred_wind.reshape(df_test_true_wind.shape)\n","#  y_pred_wind_lst_ffnn.append(y_pred_wind)\n","#  rmse_wind, mae_wind, wmape, mase_wind, nrmse_wind, acc_wind, acc_wind_90 = evaluate_predictions_windows(df_test_true_wind, y_pred_wind, test_y = Xtest_fridge_wind, testing=False)\n","\n","ffnn_lst_results = [ffnn_rmse, ffnn_mae, ffnn_wmape, ffnn_mase, ffnn_nrmse, ffnn_acc, ffnn_acc_90, ffnn_train_time]\n","evaluation_dict['FFNN'] = ffnn_lst_results"]},{"cell_type":"markdown","metadata":{"id":"5dMZo3SnpDXn"},"source":["#### CNN-LSTM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8t--HyhxKttb"},"outputs":[],"source":["start = time.time()\n","cnn_lstm_result_lst = []\n","for i in [42, 10, 567, 239, 400, 1390, 380, 9, 27, 769]:\n","\n","  # Define input layers\n","  sequential_input = Input(Xtrain_arrayfe_cnn_lstm[0].shape)\n","  \n","  # Model Architecture: CNN-LSTM\n","  if Xtrain_arrayfe_cnn_lstm[0].shape[1]==1:\n","    conv1 = TimeDistributed(Conv1D(filters=conv_filter, kernel_size=kernel_size-1))(sequential_input)\n","    pl_size = 1\n","  else:\n","    conv1 = TimeDistributed(Conv1D(filters=conv_filter, kernel_size=kernel_size))(sequential_input)\n","  bn1 = TimeDistributed(BatchNormalization())(conv1)\n","  lr1 = TimeDistributed(LeakyReLU(alpha=lr_alpha))(bn1)\n","  drpt1 = TimeDistributed(Dropout(drpt_rate_cnn))(lr1)\n","  mxpl1 = TimeDistributed(MaxPooling1D(pool_size=pl_size))(drpt1)\n","  conv2 = TimeDistributed(Conv1D(filters=conv_filter, kernel_size=int(kernel_size-1)))(mxpl1)\n","  bn2 = TimeDistributed(BatchNormalization())(conv1)\n","  lr2 = TimeDistributed(LeakyReLU(alpha=lr_alpha))(bn2)\n","  drpt2 = TimeDistributed(Dropout(drpt_rate_cnn))(lr2)\n","  mxpl2 = TimeDistributed(MaxPooling1D(pool_size=pl_size))(drpt2)\n","  flat1 = TimeDistributed(Flatten())(mxpl2)\n","  lstm_l = LSTM(hidd_dim_cnn_lstm)(flat1)\n","  lrout = LeakyReLU(alpha=lr_alpha)(lstm_l)\n","  outputs = Dense(24)(lrout)\n","  \n","  # Define the model\n","  cnn_lstm = Model(inputs=sequential_input, outputs=outputs)\n","  cnn_lstm.compile(loss=loss, optimizer=optimizer1)\n","\n","\n","  seed_value = i\n","  os.environ['PYTHONHASHSEED']=str(seed_value)\n","  random.seed(seed_value)\n","  np.random.seed(seed_value)\n","  tf.compat.v1.set_random_seed(seed_value)\n","  cnn_lstm.fit(Xtrain_arrayfe_cnn_lstm, ytrain_arrayfe, validation_data = (Xvalid_arrayfe_cnn_lstm, yvalid_arrayfe), epochs = epochs)\n","  y_pred = cnn_lstm.predict(Xtest_cnn_lstm)\n","  cnn_lstm_result_lst.append(y_pred)\n","\n","end = time.time()\n","\n","print(\"Time elapsed:\", end - start)\n","\n","cnn_train_time = end - start"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzC7goXNV8cD"},"outputs":[],"source":["# calculate differences to original data for many models:\n","rmse_lst, mae_lst, wmape_lst, mase_lst, nrmse_lst, acc_lst, acc_lst_90 = print_originalvalues_results(cnn_lstm_result_lst, df_test_true, Xtest_fridge)\n","\n","cnn_rmse = sum(rmse_lst)/len(rmse_lst)\n","cnn_mae = sum(mae_lst)/len(mae_lst)\n","cnn_wmape = sum(wmape_lst)/len(wmape_lst)\n","cnn_mase = sum(mase_lst)/len(mase_lst)\n","cnn_nrmse = sum(nrmse_lst)/len(nrmse_lst)\n","cnn_acc = sum(acc_lst)/len(acc_lst)\n","cnn_acc_90 = sum(acc_lst_90)/len(acc_lst_90)\n","\n","print('Mean RMSE, MAE, WMAPE, MASE, NRMSE, ACCURACY:')\n","print(f'{cnn_rmse:.5f}/{cnn_mae:.5f}/{cnn_wmape:.5f}/{cnn_mase:.5f}/{cnn_nrmse:.5f}/{cnn_acc:.5f}')\n","\n","cnn_lst_results = [cnn_rmse, cnn_mae, cnn_wmape, cnn_mase, cnn_nrmse, cnn_acc, cnn_acc_90, cnn_train_time]\n","evaluation_dict['CNN'] = cnn_lst_results"]},{"cell_type":"markdown","metadata":{"id":"USEIeA5AnouH"},"source":["#### Results Summary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aUcQbJ4P1S1C","executionInfo":{"status":"ok","timestamp":1650667373092,"user_tz":-120,"elapsed":212,"user":{"displayName":"Antonia Deeplearning","userId":"03469055858719879397"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"52fd56ce-a3f0-4d7a-98f7-e22af3dd9e13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Scores: RMSE/MAE/MASE/nRMSE/Accuracy/Time\n","Model XGBoost scores: 17.64/14.453/0.8552/0.1742/0.8010/124\n","Model MSVR scores: 15.72/13.517/0.7998/0.1552/0.7768/601\n"]}],"source":["## Get all results together:\n","print('Scores: RMSE/MAE/MASE/nRMSE/Accuracy/Time')\n","for item in evaluation_dict:\n","  print(f'Model {item} scores: {evaluation_dict[item][0]:.2f}/{evaluation_dict[item][1]:.3f}/{evaluation_dict[item][3]:.4f}/{evaluation_dict[item][4]:.4f}/{evaluation_dict[item][5]:.4f}/{evaluation_dict[item][7]:.0f}')\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["2Nvti6BqQehd","kdiZiISmPHWB","jfjbmAIeQVo1","ZcUOBBlWc9yX","4Yhu8nqrGA54","-fFZ-DLLkkfu","MdpYMGImIEA6","IDtbJvKNGfnx","5dMZo3SnpDXn"],"name":"run_experiments.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}